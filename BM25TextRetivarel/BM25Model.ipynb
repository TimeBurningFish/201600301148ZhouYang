{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25 检索模型\n",
    "## 流程\n",
    "- 数据预处理\n",
    "- 建立到排索引\n",
    "- 实现OR操作\n",
    "- 实现f(q,d）核函数\n",
    "- 查询操作返回topK relevance 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "- 解析json格式数据得到dict_list\n",
    "- 去非关键性标点，换行符等，split\n",
    "- 建立vocab（Tokenize）\n",
    "- 保存结果数据\n",
    "###### 注意这里的 text texts的顺序相同 vbocab 一个文档只记录一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data/tweets.txt','r+')\n",
    "lines = f.readlines()\n",
    "text = []\n",
    "docid = []\n",
    "for l in lines:\n",
    "    text.append(json.loads(l)['text'])\n",
    "    docid.append(json.loads(l)['tweetId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "symbol = [',',':','_','!','\\\"','*','>','<','@','~','-','(',')','%','=','\\\\','^'\n",
    "          ,'&','|','#','$','[',']','+',':','#','|'] \n",
    "for l in text:\n",
    "    trantab = str.maketrans({key: None for key in string.punctuation})\n",
    "    j = l.translate(trantab)\n",
    "    j = j.split()\n",
    "    texts.append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for line in texts:\n",
    "    line_set = set(line) ##去重\n",
    "    for word in line_set:       \n",
    "            if vocab.get(word) == None:\n",
    "                vocab[word] = 1\n",
    "            else :\n",
    "                vocab[word] += 1\n",
    "v_tuple = sorted(vocab.items(),key = lambda x:x[1],reverse=True) ##从大到小排序\n",
    "vocab = {}\n",
    "for t in v_tuple:\n",
    "    vocab[t[0]] = t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data/vocab.txt','w+')\n",
    "line_str = json.dumps(vocab)\n",
    "f.write(line_str)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立倒排索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2inverted_index = {} ##一个字典 存frequency 一个字典存 每个词出现的docID\n",
    "keys = list(vocab.keys())\n",
    "for k in keys:\n",
    "    word2inverted_index[k] = []##初始化\n",
    "\n",
    "for i,line in enumerate(texts):\n",
    "    line_set = set(line) ##去重\n",
    "    for word in line_set:\n",
    "        word2inverted_index[word].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立IDF字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_idf = [len(word2inverted_index[k]) for k in keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算avgD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lend = [len(t) for t in texts]\n",
    "avgD = sum(lend)/len(lend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR_op(l1,l2):\n",
    "    ##两个有序链表 返回并集 docID 并且去重 有序\n",
    "    result = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while(i<len(l1) and j < len(l2)):\n",
    "        if l1[i] == l2[j]:\n",
    "            result.append(l1[i])\n",
    "            \n",
    "            i += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            if l1[i] < l2[j]:\n",
    "                result.append(l1[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                result.append(l2[j])\n",
    "                j += 1\n",
    "    result.extend(l1[i:])\n",
    "    result.extend(l2[j:])\n",
    "    ##注意这里注意剩余\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现计算relevance 的核函数f(q,d)\n",
    "- q:列表 query\n",
    "- d:列表 document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "ki = 5\n",
    "b = 0.5\n",
    "M = len(texts)\n",
    "def count(w,d):\n",
    "    i = 0\n",
    "    for j in d:\n",
    "        if  w== j:\n",
    "            i+=1\n",
    "    return i\n",
    "\n",
    "def fqd(q,d):\n",
    "    result_relevance  = 0\n",
    "    qs = set(q)#去重\n",
    "    middel = ki * (1-b + b*len(d)/avgD)\n",
    "    for w in qs:\n",
    "        if w in d:\n",
    "            if w in keys:\n",
    "                result_relevance += count(w,q)*(ki+1)*count(w,d)/(count(w,d)+middel)*math.log2((M+1.0)/d_idf[keys.index(w)])\n",
    "    return result_relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现topk查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_stop = open('stopword','r+')\n",
    "stopwords = f_stop.readlines()\n",
    "stopwords = [s.strip('\\n') for s in stopwords]\n",
    "f_stop.close()\n",
    "\n",
    "def query_topk(Q,k = 20):\n",
    "    \n",
    "    trantab = str.maketrans({key: None for key in string.punctuation})\n",
    "    j = Q.translate(trantab)\n",
    "    j = j.split()\n",
    "    qs = []\n",
    "    for qqq in j:\n",
    "        if qqq not in stopwords:\n",
    "            qs.append(qqq)\n",
    "        \n",
    "    ls = [word2inverted_index[w]  for w in qs if w in keys]\n",
    "    ds = []\n",
    "    for l in ls:\n",
    "        ds = OR_op(ds,l)\n",
    "    relevace = {}## d - > relevance\n",
    "    for d in ds:\n",
    "        relevace[d] = fqd(qs,texts[d])\n",
    "    \n",
    "    v_tuple = sorted(relevace.items(),key = lambda x:x[1],reverse=True) ##从大到小排序\n",
    "    relevace = {}\n",
    "    for t in v_tuple:\n",
    "        relevace[t[0]] = t[1]\n",
    "    kk = list(relevace.keys())\n",
    "    k = min(k,len(kk))\n",
    "    return [texts[kk[i]] for i in range(k)]\n",
    "\n",
    "def query_topk_id(Q,k = 20):\n",
    "    \n",
    "    trantab = str.maketrans({key: None for key in string.punctuation})\n",
    "    j = Q.translate(trantab)\n",
    "    j = j.split()\n",
    "    qs = []\n",
    "    for qqq in j:\n",
    "        if qqq not in stopwords:\n",
    "            qs.append(qqq)\n",
    "    \n",
    "    ls = [word2inverted_index[w]  for w in qs if w in keys]\n",
    "    ds = []\n",
    "    for l in ls:\n",
    "        ds = OR_op(ds,l)\n",
    "    relevace = {}## d - > relevance\n",
    "    for d in ds:\n",
    "        relevace[d] = fqd(qs,texts[d])\n",
    "    \n",
    "    v_tuple = sorted(relevace.items(),key = lambda x:x[1],reverse=True) ##从大到小排序\n",
    "    relevace = {}\n",
    "    for t in v_tuple:\n",
    "        relevace[t[0]] = t[1]\n",
    "    kk = list(relevace.keys())\n",
    "    k = min(k,len(kk))\n",
    "    return [docid[kk[i]] for i in range(k)]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inred( s ):\n",
    "    return\"%s[31;2m%s%s[0m\"%(chr(27), s, chr(27))\n",
    "def print_line_with_important(line , Q):\n",
    "    trantab = str.maketrans({key: None for key in string.punctuation})\n",
    "    j = Q.translate(trantab)\n",
    "    j = j.split()\n",
    "    s_line = j\n",
    "    to_be_print = ''\n",
    "    for l in line:\n",
    "        if l in s_line:\n",
    "            to_be_print += inred(l) +' ' ##这里强调\n",
    "        else :\n",
    "            to_be_print += l + ' '\n",
    "    print(to_be_print)\n",
    "\n",
    "def print_top_k(Q):\n",
    "    for line in query_topk(Q):\n",
    "        print_line_with_important(line,Q)\n",
    "\n",
    "def save_top_k(Qid,Q,k,f):\n",
    "    for l in query_topk_id(Q,k):\n",
    "        f.write(str(Qid)+\" \"+str(l)+\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('query.txt','r+')\n",
    "Soup = BeautifulSoup(f,'lxml'); \n",
    "query = Soup.select('top > query')\n",
    "query = [q.getText() for q in query]\n",
    "queryid = Soup.select('top > num')\n",
    "queryid = [q.getText()[11:-1] for q in queryid]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('result.txt','w+')\n",
    "for i,q in  enumerate(query):\n",
    "    save_top_k(queryid[i],q,200,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am \u001b[31;2mnaming\u001b[0m this \u001b[31;2mstorm\u001b[0m Finding \u001b[31;2mNemo\u001b[0m wheresheat \n",
      "They are \u001b[31;2mnaming\u001b[0m snow storms now Winter \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m Really \n",
      "When the hell did we start \u001b[31;2mnaming\u001b[0m nonhurricanes Storm \u001b[31;2mNemo\u001b[0m Isnt that a cartoon fish How scary is a \u001b[31;2mstorm\u001b[0m named \u001b[31;2mNemo\u001b[0m \n",
      "Did they name the \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m so every news and weather outlet can use the phrase Finding \u001b[31;2mNemo\u001b[0m when tracking the \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m \n",
      "NormansCat \u001b[31;2mNemo\u001b[0m They named the \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m Wow awesome \n",
      "Wait they named this \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m As in Finding \u001b[31;2mNemo\u001b[0m \n",
      "Winter \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m Who gives names to these storms and when did they start \u001b[31;2mnaming\u001b[0m winter storms \n",
      "httptcoheJAhFSp is kind of undercutting their terrifying YOU MUST PREPARE NOW headlines by \u001b[31;2mnaming\u001b[0m the \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m \n",
      "Nycmoon this \u001b[31;2mnaming\u001b[0m of winter storms is so dumb \u001b[31;2mNemo\u001b[0m Really They actually named it \u001b[31;2mNemo\u001b[0m \n",
      "Who names the worst winter \u001b[31;2mstorm\u001b[0m ever \u001b[31;2mNemo\u001b[0m Why not Godzilla How about not \u001b[31;2mnaming\u001b[0m winter storms tcot NRA \n",
      "Since when did they start \u001b[31;2mnaming\u001b[0m snowstorms and why \u001b[31;2mNemo\u001b[0m that names too sweet for a historic crippling \u001b[31;2mstorm\u001b[0m \n",
      "Why would they name this \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m Whats next super \u001b[31;2mstorm\u001b[0m WallE \n",
      "Winter \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m httptcoSVlr8SXr \n",
      "\u001b[31;2mNemo\u001b[0m Orko Are we \u001b[31;2mnaming\u001b[0m storms or cartoon characters \n",
      "Lol Winter \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m lol \n",
      "Dont call this \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m or \u001b[31;2mNemo\u001b[0m or you let the Weather Channel win httptco9oBu4eQX \n",
      "Theyre \u001b[31;2mnaming\u001b[0m this \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m I just looked it up it means no one 1 choose a more hopeful baby name httptcoZ5JEFR0q \n",
      "Why is this \u001b[31;2mstorm\u001b[0m named \u001b[31;2mNemo\u001b[0m \n",
      "They named the \u001b[31;2mstorm\u001b[0m after \u001b[31;2mNemo\u001b[0m \n",
      "The named the \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m tybg \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' commentary on naming storm Nemo '"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_top_k(query[4])\n",
    "query[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对结果进行evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_hw4.eval_hw4 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我的结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: 171 ,AP: 0.751931648831\n",
      "query: 172 ,AP: 0.596615551401\n",
      "query: 173 ,AP: 0.133333333333\n",
      "query: 174 ,AP: 0.573125485625\n",
      "query: 175 ,AP: 0.686814188789\n",
      "query: 176 ,AP: 0.618201010357\n",
      "query: 177 ,AP: 0.711951809919\n",
      "query: 178 ,AP: 0.841574334187\n",
      "query: 179 ,AP: 0.525652825887\n",
      "query: 180 ,AP: 0.278287629881\n",
      "query: 181 ,AP: 0.888888888889\n",
      "query: 182 ,AP: 0.3861003861\n",
      "query: 183 ,AP: 0.755045992217\n",
      "query: 184 ,AP: 0.697272570827\n",
      "query: 185 ,AP: 0.805110874964\n",
      "query: 186 ,AP: 0.73929657782\n",
      "query: 187 ,AP: 0.868070082366\n",
      "query: 188 ,AP: 0.469349245212\n",
      "query: 189 ,AP: 0.0432049675471\n",
      "query: 190 ,AP: 0.46514161684\n",
      "query: 191 ,AP: 0.553197303165\n",
      "query: 192 ,AP: 0.457157081692\n",
      "query: 193 ,AP: 0.526663705071\n",
      "query: 194 ,AP: 0.372395833333\n",
      "query: 195 ,AP: 0.424510787276\n",
      "query: 196 ,AP: 0.761638118757\n",
      "query: 197 ,AP: 0.79110887911\n",
      "query: 198 ,AP: 0.495060370516\n",
      "query: 199 ,AP: 0.472034088134\n",
      "query: 200 ,AP: 0.663251218227\n",
      "query: 201 ,AP: 0.713372587697\n",
      "query: 202 ,AP: 0.795626081736\n",
      "query: 203 ,AP: 0.0568124251635\n",
      "query: 204 ,AP: 0.741777298385\n",
      "query: 205 ,AP: 0.805237115683\n",
      "query: 206 ,AP: 0.365630474283\n",
      "query: 207 ,AP: 0.771361472034\n",
      "query: 208 ,AP: 0.44925991394\n",
      "query: 209 ,AP: 0.276600289398\n",
      "query: 210 ,AP: 0.666997568571\n",
      "query: 211 ,AP: 0.662115687973\n",
      "query: 212 ,AP: 0.854827791052\n",
      "query: 213 ,AP: 0.62523698803\n",
      "query: 214 ,AP: 0.89242845974\n",
      "query: 215 ,AP: 0.480491538127\n",
      "query: 216 ,AP: 0.509098948273\n",
      "query: 217 ,AP: 0.376938125826\n",
      "query: 218 ,AP: 0.160257998168\n",
      "query: 219 ,AP: 0.491425805338\n",
      "query: 220 ,AP: 0.179911616162\n",
      "query: 221 ,AP: 0.397614314115\n",
      "query: 222 ,AP: 0.658311644918\n",
      "query: 223 ,AP: 0.317853586542\n",
      "query: 224 ,AP: 0.5\n",
      "query: 225 ,AP: 0.992424242424\n",
      "MAP = 0.565338152361\n",
      "query 171 , NDCG:  0.832026058047054\n",
      "query 172 , NDCG:  0.8933573777637464\n",
      "query 173 , NDCG:  0.647939623894138\n",
      "query 174 , NDCG:  0.7242323397691163\n",
      "query 175 , NDCG:  0.7774476800512304\n",
      "query 176 , NDCG:  0.5855962874814986\n",
      "query 177 , NDCG:  0.7571544529619776\n",
      "query 178 , NDCG:  0.870176586687736\n",
      "query 179 , NDCG:  0.5612803674199066\n",
      "query 180 , NDCG:  0.590750696082566\n",
      "query 181 , NDCG:  0.9037236369993927\n",
      "query 182 , NDCG:  0.7674163072691148\n",
      "query 183 , NDCG:  0.8886274156489069\n",
      "query 184 , NDCG:  0.5563664051573255\n",
      "query 185 , NDCG:  0.8401560186758996\n",
      "query 186 , NDCG:  0.8502743389691\n",
      "query 187 , NDCG:  0.7371910960249517\n",
      "query 188 , NDCG:  0.5428076042448582\n",
      "query 189 , NDCG:  0.1280642518068232\n",
      "query 190 , NDCG:  0.6195323101433512\n",
      "query 191 , NDCG:  0.7231369247814486\n",
      "query 192 , NDCG:  0.6453553084008472\n",
      "query 193 , NDCG:  0.5352491379378426\n",
      "query 194 , NDCG:  0.5321798053481208\n",
      "query 195 , NDCG:  0.5089350066884523\n",
      "query 196 , NDCG:  0.7541599828638935\n",
      "query 197 , NDCG:  0.7806005876854659\n",
      "query 198 , NDCG:  0.6105965837711599\n",
      "query 199 , NDCG:  0.8477537196281778\n",
      "query 200 , NDCG:  0.7864537862179347\n",
      "query 201 , NDCG:  0.8069752578086178\n",
      "query 202 , NDCG:  0.8204562034338198\n",
      "query 203 , NDCG:  0.1351596156338649\n",
      "query 204 , NDCG:  0.8069809243972833\n",
      "query 205 , NDCG:  0.837519822195502\n",
      "query 206 , NDCG:  0.420336386705024\n",
      "query 207 , NDCG:  0.7889614413440238\n",
      "query 208 , NDCG:  0.7116254071269844\n",
      "query 209 , NDCG:  0.682733592317878\n",
      "query 210 , NDCG:  0.7221936598517699\n",
      "query 211 , NDCG:  0.5867736686305127\n",
      "query 212 , NDCG:  0.8465926194556943\n",
      "query 213 , NDCG:  0.8179158365857889\n",
      "query 214 , NDCG:  0.8717164071813494\n",
      "query 215 , NDCG:  0.6300461927122866\n",
      "query 216 , NDCG:  0.6775353197651431\n",
      "query 217 , NDCG:  0.5569878528780521\n",
      "query 218 , NDCG:  0.46249758135452\n",
      "query 219 , NDCG:  0.49781472882191324\n",
      "query 220 , NDCG:  0.257674022728908\n",
      "query 221 , NDCG:  0.8254287741139867\n",
      "query 222 , NDCG:  0.5734049788720491\n",
      "query 223 , NDCG:  0.41890981288073303\n",
      "query 224 , NDCG:  0.6416075333609556\n",
      "query 225 , NDCG:  0.8644792606127056\n",
      "NDCG = 0.673833974531\n"
     ]
    }
   ],
   "source": [
    "k = 200\n",
    "# query relevance file\n",
    "file_qrels_path = 'eval_hw4/qrels.txt'\n",
    "# qrels_dict = {query_id:{doc_id:gain, doc_id:gain, ...}, ...}\n",
    "qrels_dict = generate_tweetid_gain(file_qrels_path)\n",
    "# ur result, format is in function read_tweetid_test, or u can write by ur own\n",
    "file_test_path = 'result.txt'\n",
    "# test_dict = {query_id:[doc_id, doc_id, ...], ...}\n",
    "test_dict = read_tweetid_test(file_test_path)\n",
    "MAP = MAP_eval(qrels_dict, test_dict, k)\n",
    "print('MAP', ' = ', MAP, sep='')\n",
    "NDCG = NDCG_eval(qrels_dict, test_dict, k)\n",
    "print('NDCG', ' = ', NDCG, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对比结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: 171 ,AP: 0.94980405976\n",
      "query: 172 ,AP: 0.682593856655\n",
      "query: 173 ,AP: 0.997813620072\n",
      "query: 174 ,AP: 0.567534780035\n",
      "query: 175 ,AP: 0.778210116732\n",
      "query: 176 ,AP: 0.827412933877\n",
      "query: 177 ,AP: 0.465198145065\n",
      "query: 178 ,AP: 0.915379647212\n",
      "query: 179 ,AP: 0.971163259061\n",
      "query: 180 ,AP: 0.220973360037\n",
      "query: 181 ,AP: 1.0\n",
      "query: 182 ,AP: 0.3861003861\n",
      "query: 183 ,AP: 0.851063829787\n",
      "query: 184 ,AP: 0.953864220785\n",
      "query: 185 ,AP: 0.565475418125\n",
      "query: 186 ,AP: 0.986660079051\n",
      "query: 187 ,AP: 0.909298308663\n",
      "query: 188 ,AP: 0.818160705003\n",
      "query: 189 ,AP: 0.37571536147\n",
      "query: 190 ,AP: 0.93514660103\n",
      "query: 191 ,AP: 0.878837663343\n",
      "query: 192 ,AP: 1.0\n",
      "query: 193 ,AP: 1.0\n",
      "query: 194 ,AP: 0.977071661998\n",
      "query: 195 ,AP: 0.515391404296\n",
      "query: 196 ,AP: 1.0\n",
      "query: 197 ,AP: 0.998919203817\n",
      "query: 198 ,AP: 1.0\n",
      "query: 199 ,AP: 0.475059382423\n",
      "query: 200 ,AP: 0.712377070122\n",
      "query: 201 ,AP: 0.740740740741\n",
      "query: 202 ,AP: 0.924676840984\n",
      "query: 203 ,AP: 0.713699248416\n",
      "query: 204 ,AP: 0.899091826819\n",
      "query: 205 ,AP: 0.967535476892\n",
      "query: 206 ,AP: 0.910312226521\n",
      "query: 207 ,AP: 0.960750573412\n",
      "query: 208 ,AP: 0.607902735562\n",
      "query: 209 ,AP: 0.328947368421\n",
      "query: 210 ,AP: 0.963534467482\n",
      "query: 211 ,AP: 0.252260351262\n",
      "query: 212 ,AP: 0.864406234706\n",
      "query: 213 ,AP: 0.796812749004\n",
      "query: 214 ,AP: 0.86230442666\n",
      "query: 215 ,AP: 0.602409638554\n",
      "query: 216 ,AP: 0.828987746145\n",
      "query: 217 ,AP: 0.999641622797\n",
      "query: 218 ,AP: 0.606060606061\n",
      "query: 219 ,AP: 0.337907550056\n",
      "query: 220 ,AP: 0.613822662115\n",
      "query: 221 ,AP: 0.397614314115\n",
      "query: 222 ,AP: 0.69352079397\n",
      "query: 223 ,AP: 0.994074673605\n",
      "query: 224 ,AP: 0.517873237873\n",
      "query: 225 ,AP: 0.992006355326\n",
      "MAP = 0.7653112644\n",
      "query 171 , NDCG:  0.9398543518229351\n",
      "query 172 , NDCG:  0.9633915005168991\n",
      "query 173 , NDCG:  0.8787194969898994\n",
      "query 174 , NDCG:  0.4307012038436227\n",
      "query 175 , NDCG:  0.8270509709611499\n",
      "query 176 , NDCG:  0.7642638365304593\n",
      "query 177 , NDCG:  0.4031227315582503\n",
      "query 178 , NDCG:  0.9149546965165309\n",
      "query 179 , NDCG:  0.9092261961802077\n",
      "query 180 , NDCG:  0.5611568950054991\n",
      "query 181 , NDCG:  0.9083280342057781\n",
      "query 182 , NDCG:  0.8840328408126277\n",
      "query 183 , NDCG:  0.9102744814242358\n",
      "query 184 , NDCG:  0.8445138195987467\n",
      "query 185 , NDCG:  0.5651704753561145\n",
      "query 186 , NDCG:  0.9174314725664987\n",
      "query 187 , NDCG:  0.8568815395907531\n",
      "query 188 , NDCG:  0.834462410887587\n",
      "query 189 , NDCG:  0.11401721726142679\n",
      "query 190 , NDCG:  0.9087219839232467\n",
      "query 191 , NDCG:  0.8333343147042753\n",
      "query 192 , NDCG:  0.8691210155951211\n",
      "query 193 , NDCG:  0.870741244990849\n",
      "query 194 , NDCG:  0.9169177532845512\n",
      "query 195 , NDCG:  0.6639616560278855\n",
      "query 196 , NDCG:  0.985504817367503\n",
      "query 197 , NDCG:  0.9366145863919296\n",
      "query 198 , NDCG:  0.8656740779203047\n",
      "query 199 , NDCG:  0.8277140391541248\n",
      "query 200 , NDCG:  0.8711501907272423\n",
      "query 201 , NDCG:  0.88743362829229\n",
      "query 202 , NDCG:  0.9107545852790405\n",
      "query 203 , NDCG:  0.5568543671092813\n",
      "query 204 , NDCG:  0.8819018257589796\n",
      "query 205 , NDCG:  0.9252555622479198\n",
      "query 206 , NDCG:  0.8077691566644618\n",
      "query 207 , NDCG:  0.8228677166265421\n",
      "query 208 , NDCG:  0.8581807002957024\n",
      "query 209 , NDCG:  0.6939250586840917\n",
      "query 210 , NDCG:  0.9144104200186212\n",
      "query 211 , NDCG:  0.046597135518310455\n",
      "query 212 , NDCG:  0.8282008992831403\n",
      "query 213 , NDCG:  0.9622674183267285\n",
      "query 214 , NDCG:  0.7936758103546756\n",
      "query 215 , NDCG:  0.6952067228508936\n",
      "query 216 , NDCG:  0.8590276180495913\n",
      "query 217 , NDCG:  0.9215234698355937\n",
      "query 218 , NDCG:  0.9229337070288905\n",
      "query 219 , NDCG:  0.498155912259978\n",
      "query 220 , NDCG:  0.5674800702438964\n",
      "query 221 , NDCG:  0.9112435588714188\n",
      "query 222 , NDCG:  0.618683715017237\n",
      "query 223 , NDCG:  0.9063275712084274\n",
      "query 224 , NDCG:  0.3773185814513307\n",
      "query 225 , NDCG:  0.9706077927297266\n",
      "NDCG = 0.78392023374\n"
     ]
    }
   ],
   "source": [
    "k = 200\n",
    "# query relevance file\n",
    "file_qrels_path = 'eval_hw4/qrels.txt'\n",
    "# qrels_dict = {query_id:{doc_id:gain, doc_id:gain, ...}, ...}\n",
    "qrels_dict = generate_tweetid_gain(file_qrels_path)\n",
    "# ur result, format is in function read_tweetid_test, or u can write by ur own\n",
    "file_test_path = 'eval_hw4/hisresult.txt'\n",
    "# test_dict = {query_id:[doc_id, doc_id, ...], ...}\n",
    "test_dict = read_tweetid_test(file_test_path)\n",
    "MAP = MAP_eval(qrels_dict, test_dict, k)\n",
    "print('MAP', ' = ', MAP, sep='')\n",
    "NDCG = NDCG_eval(qrels_dict, test_dict, k)\n",
    "print('NDCG', ' = ', NDCG, sep='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
