{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25 检索模型\n",
    "## 流程\n",
    "- 数据预处理\n",
    "- 建立到排索引\n",
    "- 实现OR操作\n",
    "- 实现f(q,d）核函数\n",
    "- 查询操作返回topK relevance 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "- 解析json格式数据得到dict_list\n",
    "- 去非关键性标点，换行符等，split\n",
    "- 建立vocab（Tokenize）\n",
    "- 保存结果数据\n",
    "###### 注意这里的 text texts的顺序相同 vbocab 一个文档只记录一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data/tweets.txt','r+')\n",
    "lines = f.readlines()\n",
    "text = []\n",
    "docid = []\n",
    "for l in lines:\n",
    "    text.append(json.loads(l)['text'])\n",
    "    docid.append(json.loads(l)['tweetId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "symbol = [',',':','_','!','\\\"','*','>','<','@','~','-','(',')','%','=','\\\\','^'\n",
    "          ,'&','|','#','$','[',']','+',':','#','|'] \n",
    "for l in text:\n",
    "    trantab = str.maketrans({key: None for key in string.punctuation})\n",
    "    j = l.translate(trantab)\n",
    "    j = j.split()\n",
    "    texts.append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for line in texts:\n",
    "    line_set = set(line) ##去重\n",
    "    for word in line_set:       \n",
    "            if vocab.get(word) == None:\n",
    "                vocab[word] = 1\n",
    "            else :\n",
    "                vocab[word] += 1\n",
    "v_tuple = sorted(vocab.items(),key = lambda x:x[1],reverse=True) ##从大到小排序\n",
    "vocab = {}\n",
    "for t in v_tuple:\n",
    "    vocab[t[0]] = t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data/vocab.txt','w+')\n",
    "line_str = json.dumps(vocab)\n",
    "f.write(line_str)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立倒排索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2inverted_index = {} ##一个字典 存frequency 一个字典存 每个词出现的docID\n",
    "keys = list(vocab.keys())\n",
    "for k in keys:\n",
    "    word2inverted_index[k] = []##初始化\n",
    "\n",
    "for i,line in enumerate(texts):\n",
    "    line_set = set(line) ##去重\n",
    "    for word in line_set:\n",
    "        word2inverted_index[word].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立IDF字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_idf = [len(word2inverted_index[k]) for k in keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算avgD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lend = [len(t) for t in texts]\n",
    "avgD = sum(lend)/len(lend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR_op(l1,l2):\n",
    "    ##两个有序链表 返回并集 docID 并且去重 有序\n",
    "    result = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while(i<len(l1) and j < len(l2)):\n",
    "        if l1[i] == l2[j]:\n",
    "            result.append(l1[i])\n",
    "            \n",
    "            i += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            if l1[i] < l2[j]:\n",
    "                result.append(l1[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                result.append(l2[j])\n",
    "                j += 1\n",
    "    result.extend(l1[i:])\n",
    "    result.extend(l2[j:])\n",
    "    ##注意这里注意剩余\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现计算relevance 的核函数f(q,d)\n",
    "- q:列表 query\n",
    "- d:列表 document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "ki = 5\n",
    "b = 0.5\n",
    "M = len(texts)\n",
    "def count(w,d):\n",
    "    i = 0\n",
    "    for j in d:\n",
    "        if  w== j:\n",
    "            i+=1\n",
    "    return i\n",
    "\n",
    "def fqd(q,d):\n",
    "    result_relevance  = 0\n",
    "    qs = set(q)#去重\n",
    "    middel = ki * (1-b + b*len(d)/avgD)\n",
    "    for w in qs:\n",
    "        if w in d:\n",
    "            result_relevance += count(w,q)*(ki+1)*count(w,d)/(count(w,d)+middel)*math.log2((M+1.0)/d_idf[keys.index('House')])\n",
    "    return result_relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现topk查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_stop = open('stopword','r+')\n",
    "stopwords = f_stop.readlines()\n",
    "stopwords = [s.strip('\\n') for s in stopwords]\n",
    "f_stop.close()\n",
    "\n",
    "def query_topk(Q,k = 20):\n",
    "    \n",
    "    trantab = str.maketrans({key: None for key in string.punctuation})\n",
    "    j = Q.translate(trantab)\n",
    "    j = j.split()\n",
    "    qs = []\n",
    "    for qqq in j:\n",
    "        if qqq not in stopwords:\n",
    "            qs.append(qqq)\n",
    "        \n",
    "    ls = [word2inverted_index[w]  for w in qs if w in keys]\n",
    "    ds = []\n",
    "    for l in ls:\n",
    "        ds = OR_op(ds,l)\n",
    "    relevace = {}## d - > relevance\n",
    "    for d in ds:\n",
    "        relevace[d] = fqd(qs,texts[d])\n",
    "    \n",
    "    v_tuple = sorted(relevace.items(),key = lambda x:x[1],reverse=True) ##从大到小排序\n",
    "    relevace = {}\n",
    "    for t in v_tuple:\n",
    "        relevace[t[0]] = t[1]\n",
    "    kk = list(relevace.keys())\n",
    "    k = min(k,len(kk))\n",
    "    return [texts[kk[i]] for i in range(k)]\n",
    "\n",
    "def query_topk_id(Q,k = 20):\n",
    "    \n",
    "    trantab = str.maketrans({key: None for key in string.punctuation})\n",
    "    j = Q.translate(trantab)\n",
    "    j = j.split()\n",
    "    qs = []\n",
    "    for qqq in j:\n",
    "        if qqq not in stopwords:\n",
    "            qs.append(qqq)\n",
    "    \n",
    "    ls = [word2inverted_index[w]  for w in qs if w in keys]\n",
    "    ds = []\n",
    "    for l in ls:\n",
    "        ds = OR_op(ds,l)\n",
    "    relevace = {}## d - > relevance\n",
    "    for d in ds:\n",
    "        relevace[d] = fqd(qs,texts[d])\n",
    "    \n",
    "    v_tuple = sorted(relevace.items(),key = lambda x:x[1],reverse=True) ##从大到小排序\n",
    "    relevace = {}\n",
    "    for t in v_tuple:\n",
    "        relevace[t[0]] = t[1]\n",
    "    kk = list(relevace.keys())\n",
    "    k = min(k,len(kk))\n",
    "    return [docid[kk[i]] for i in range(k)]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inred( s ):\n",
    "    return\"%s[31;2m%s%s[0m\"%(chr(27), s, chr(27))\n",
    "def print_line_with_important(line , Q):\n",
    "    trantab = str.maketrans({key: None for key in string.punctuation})\n",
    "    j = Q.translate(trantab)\n",
    "    j = j.split()\n",
    "    s_line = j\n",
    "    to_be_print = ''\n",
    "    for l in line:\n",
    "        if l in s_line:\n",
    "            to_be_print += inred(l) +' ' ##这里强调\n",
    "        else :\n",
    "            to_be_print += l + ' '\n",
    "    print(to_be_print)\n",
    "\n",
    "def print_top_k(Q):\n",
    "    for line in query_topk(Q):\n",
    "        print_line_with_important(line,Q)\n",
    "\n",
    "def save_top_k(Qid,Q,k,f):\n",
    "    for l in query_topk_id(Q,k):\n",
    "        f.write(str(Qid)+\" \"+str(l)+\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('query.txt','r+')\n",
    "Soup = BeautifulSoup(f,'lxml'); \n",
    "query = Soup.select('top > query')\n",
    "query = [q.getText() for q in query]\n",
    "queryid = Soup.select('top > num')\n",
    "queryid = [q.getText()[11:-1] for q in queryid]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('result.txt','w+')\n",
    "for i,q in  enumerate(query):\n",
    "    save_top_k(queryid[i],q,200,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am \u001b[31;2mnaming\u001b[0m this \u001b[31;2mstorm\u001b[0m Finding \u001b[31;2mNemo\u001b[0m wheresheat \n",
      "They are \u001b[31;2mnaming\u001b[0m snow storms now Winter \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m Really \n",
      "Did they name the \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m so every news and weather outlet can use the phrase Finding \u001b[31;2mNemo\u001b[0m when tracking the \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m \n",
      "NormansCat \u001b[31;2mNemo\u001b[0m They named the \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m Wow awesome \n",
      "When the hell did we start \u001b[31;2mnaming\u001b[0m nonhurricanes Storm \u001b[31;2mNemo\u001b[0m Isnt that a cartoon fish How scary is a \u001b[31;2mstorm\u001b[0m named \u001b[31;2mNemo\u001b[0m \n",
      "Wait they named this \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m As in Finding \u001b[31;2mNemo\u001b[0m \n",
      "Why would they name this \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m Whats next super \u001b[31;2mstorm\u001b[0m WallE \n",
      "Winter \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m httptcoSVlr8SXr \n",
      "Winter \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m Who gives names to these storms and when did they start \u001b[31;2mnaming\u001b[0m winter storms \n",
      "httptcoheJAhFSp is kind of undercutting their terrifying YOU MUST PREPARE NOW headlines by \u001b[31;2mnaming\u001b[0m the \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m \n",
      "Lol Winter \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m lol \n",
      "Dont call this \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m or \u001b[31;2mNemo\u001b[0m or you let the Weather Channel win httptco9oBu4eQX \n",
      "Who names the worst winter \u001b[31;2mstorm\u001b[0m ever \u001b[31;2mNemo\u001b[0m Why not Godzilla How about not \u001b[31;2mnaming\u001b[0m winter storms tcot NRA \n",
      "Since when did they start \u001b[31;2mnaming\u001b[0m snowstorms and why \u001b[31;2mNemo\u001b[0m that names too sweet for a historic crippling \u001b[31;2mstorm\u001b[0m \n",
      "Why is this \u001b[31;2mstorm\u001b[0m named \u001b[31;2mNemo\u001b[0m \n",
      "They named the \u001b[31;2mstorm\u001b[0m after \u001b[31;2mNemo\u001b[0m \n",
      "The named the \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m tybg \n",
      "pahahahahaha the \u001b[31;2mstorm\u001b[0m is named \u001b[31;2mNemo\u001b[0m \n",
      "Wait they named this \u001b[31;2mstorm\u001b[0m \u001b[31;2mNemo\u001b[0m \n",
      "Nycmoon this \u001b[31;2mnaming\u001b[0m of winter storms is so dumb \u001b[31;2mNemo\u001b[0m Really They actually named it \u001b[31;2mNemo\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' commentary on naming storm Nemo '"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_top_k(query[4])\n",
    "query[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对结果进行evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_hw4.eval_hw4 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我的结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: 171 ,AP: 0.74986098241\n",
      "query: 172 ,AP: 0.590818781865\n",
      "query: 173 ,AP: 0.118333333333\n",
      "query: 174 ,AP: 0.376227284106\n",
      "query: 175 ,AP: 0.661437817496\n",
      "query: 176 ,AP: 0.626031701265\n",
      "query: 177 ,AP: 0.649910382997\n",
      "query: 178 ,AP: 0.863223572563\n",
      "query: 179 ,AP: 0.478993432505\n",
      "query: 180 ,AP: 0.289921998097\n",
      "query: 181 ,AP: 0.888888888889\n",
      "query: 182 ,AP: 0.3861003861\n",
      "query: 183 ,AP: 0.811003459983\n",
      "query: 184 ,AP: 0.678947573203\n",
      "query: 185 ,AP: 0.647339998585\n",
      "query: 186 ,AP: 0.77135739991\n",
      "query: 187 ,AP: 0.85291125291\n",
      "query: 188 ,AP: 0.429979693289\n",
      "query: 189 ,AP: 0.0255610290093\n",
      "query: 190 ,AP: 0.452936289754\n",
      "query: 191 ,AP: 0.547970814244\n",
      "query: 192 ,AP: 0.444852892877\n",
      "query: 193 ,AP: 0.5764088747\n",
      "query: 194 ,AP: 0.32293675667\n",
      "query: 195 ,AP: 0.418122673981\n",
      "query: 196 ,AP: 0.667297050938\n",
      "query: 197 ,AP: 0.79233487683\n",
      "query: 198 ,AP: 0.538541915733\n",
      "query: 199 ,AP: 0.47204961295\n",
      "query: 200 ,AP: 0.691697952413\n",
      "query: 201 ,AP: 0.732614801206\n",
      "query: 202 ,AP: 0.794842533135\n",
      "query: 203 ,AP: 0.056366412061\n",
      "query: 204 ,AP: 0.749619248198\n",
      "query: 205 ,AP: 0.796119579938\n",
      "query: 206 ,AP: 0.317951391987\n",
      "query: 207 ,AP: 0.76758464326\n",
      "query: 208 ,AP: 0.437677629899\n",
      "query: 209 ,AP: 0.280527724993\n",
      "query: 210 ,AP: 0.715911304884\n",
      "query: 211 ,AP: 0.646134165156\n",
      "query: 212 ,AP: 0.917964742675\n",
      "query: 213 ,AP: 0.603581681367\n",
      "query: 214 ,AP: 0.892599135875\n",
      "query: 215 ,AP: 0.506944242814\n",
      "query: 216 ,AP: 0.534617847029\n",
      "query: 217 ,AP: 0.369205729599\n",
      "query: 218 ,AP: 0.182929269671\n",
      "query: 219 ,AP: 0.452753066828\n",
      "query: 220 ,AP: 0.0930009665886\n",
      "query: 221 ,AP: 0.397614314115\n",
      "query: 222 ,AP: 0.649389854285\n",
      "query: 223 ,AP: 0.277099076152\n",
      "query: 224 ,AP: 0.5\n",
      "query: 225 ,AP: 0.990973565442\n",
      "MAP = 0.554291301941\n",
      "query 171 , NDCG:  0.8313366782090886\n",
      "query 172 , NDCG:  0.8834808612708686\n",
      "query 173 , NDCG:  0.6193198671454698\n",
      "query 174 , NDCG:  0.5610142764277014\n",
      "query 175 , NDCG:  0.7561916114662682\n",
      "query 176 , NDCG:  0.5866050865614735\n",
      "query 177 , NDCG:  0.700887996455757\n",
      "query 178 , NDCG:  0.8768398928192022\n",
      "query 179 , NDCG:  0.5467551926749413\n",
      "query 180 , NDCG:  0.6034712656720087\n",
      "query 181 , NDCG:  0.9037236369993927\n",
      "query 182 , NDCG:  0.7682720027909682\n",
      "query 183 , NDCG:  0.9122654044557384\n",
      "query 184 , NDCG:  0.5349067515066023\n",
      "query 185 , NDCG:  0.765590226364567\n",
      "query 186 , NDCG:  0.8608859212150966\n",
      "query 187 , NDCG:  0.735577198010494\n",
      "query 188 , NDCG:  0.529880101136644\n",
      "query 189 , NDCG:  0.09161906267882078\n",
      "query 190 , NDCG:  0.6016683424058811\n",
      "query 191 , NDCG:  0.7226823984803619\n",
      "query 192 , NDCG:  0.6212330677678962\n",
      "query 193 , NDCG:  0.5642051325973905\n",
      "query 194 , NDCG:  0.4828791064230483\n",
      "query 195 , NDCG:  0.503693904473918\n",
      "query 196 , NDCG:  0.7200999804156943\n",
      "query 197 , NDCG:  0.7891813649223356\n",
      "query 198 , NDCG:  0.6271323192311625\n",
      "query 199 , NDCG:  0.8487641529034243\n",
      "query 200 , NDCG:  0.803299850569621\n",
      "query 201 , NDCG:  0.808474035585561\n",
      "query 202 , NDCG:  0.8200243976921797\n",
      "query 203 , NDCG:  0.1351596156338649\n",
      "query 204 , NDCG:  0.831806964710618\n",
      "query 205 , NDCG:  0.833167055362337\n",
      "query 206 , NDCG:  0.3878700310343157\n",
      "query 207 , NDCG:  0.8013664757237732\n",
      "query 208 , NDCG:  0.6813745914093406\n",
      "query 209 , NDCG:  0.6727118778907483\n",
      "query 210 , NDCG:  0.7828509743020254\n",
      "query 211 , NDCG:  0.5735700444321702\n",
      "query 212 , NDCG:  0.8889558841132458\n",
      "query 213 , NDCG:  0.8008312742038212\n",
      "query 214 , NDCG:  0.8693066561517521\n",
      "query 215 , NDCG:  0.6456436498306364\n",
      "query 216 , NDCG:  0.6846615714278947\n",
      "query 217 , NDCG:  0.5548868497013416\n",
      "query 218 , NDCG:  0.4795061432204937\n",
      "query 219 , NDCG:  0.4909183723459781\n",
      "query 220 , NDCG:  0.2307188850004417\n",
      "query 221 , NDCG:  0.8258612489896231\n",
      "query 222 , NDCG:  0.5755197049764943\n",
      "query 223 , NDCG:  0.3852987617389646\n",
      "query 224 , NDCG:  0.6416075333609556\n",
      "query 225 , NDCG:  0.8703828939706041\n",
      "NDCG = 0.665926148125\n"
     ]
    }
   ],
   "source": [
    "k = 200\n",
    "# query relevance file\n",
    "file_qrels_path = 'eval_hw4/qrels.txt'\n",
    "# qrels_dict = {query_id:{doc_id:gain, doc_id:gain, ...}, ...}\n",
    "qrels_dict = generate_tweetid_gain(file_qrels_path)\n",
    "# ur result, format is in function read_tweetid_test, or u can write by ur own\n",
    "file_test_path = 'result.txt'\n",
    "# test_dict = {query_id:[doc_id, doc_id, ...], ...}\n",
    "test_dict = read_tweetid_test(file_test_path)\n",
    "MAP = MAP_eval(qrels_dict, test_dict, k)\n",
    "print('MAP', ' = ', MAP, sep='')\n",
    "NDCG = NDCG_eval(qrels_dict, test_dict, k)\n",
    "print('NDCG', ' = ', NDCG, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对比结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: 171 ,AP: 0.94980405976\n",
      "query: 172 ,AP: 0.682593856655\n",
      "query: 173 ,AP: 0.997813620072\n",
      "query: 174 ,AP: 0.567534780035\n",
      "query: 175 ,AP: 0.778210116732\n",
      "query: 176 ,AP: 0.827412933877\n",
      "query: 177 ,AP: 0.465198145065\n",
      "query: 178 ,AP: 0.915379647212\n",
      "query: 179 ,AP: 0.971163259061\n",
      "query: 180 ,AP: 0.220973360037\n",
      "query: 181 ,AP: 1.0\n",
      "query: 182 ,AP: 0.3861003861\n",
      "query: 183 ,AP: 0.851063829787\n",
      "query: 184 ,AP: 0.953864220785\n",
      "query: 185 ,AP: 0.565475418125\n",
      "query: 186 ,AP: 0.986660079051\n",
      "query: 187 ,AP: 0.909298308663\n",
      "query: 188 ,AP: 0.818160705003\n",
      "query: 189 ,AP: 0.37571536147\n",
      "query: 190 ,AP: 0.93514660103\n",
      "query: 191 ,AP: 0.878837663343\n",
      "query: 192 ,AP: 1.0\n",
      "query: 193 ,AP: 1.0\n",
      "query: 194 ,AP: 0.977071661998\n",
      "query: 195 ,AP: 0.515391404296\n",
      "query: 196 ,AP: 1.0\n",
      "query: 197 ,AP: 0.998919203817\n",
      "query: 198 ,AP: 1.0\n",
      "query: 199 ,AP: 0.475059382423\n",
      "query: 200 ,AP: 0.712377070122\n",
      "query: 201 ,AP: 0.740740740741\n",
      "query: 202 ,AP: 0.924676840984\n",
      "query: 203 ,AP: 0.713699248416\n",
      "query: 204 ,AP: 0.899091826819\n",
      "query: 205 ,AP: 0.967535476892\n",
      "query: 206 ,AP: 0.910312226521\n",
      "query: 207 ,AP: 0.960750573412\n",
      "query: 208 ,AP: 0.607902735562\n",
      "query: 209 ,AP: 0.328947368421\n",
      "query: 210 ,AP: 0.963534467482\n",
      "query: 211 ,AP: 0.252260351262\n",
      "query: 212 ,AP: 0.864406234706\n",
      "query: 213 ,AP: 0.796812749004\n",
      "query: 214 ,AP: 0.86230442666\n",
      "query: 215 ,AP: 0.602409638554\n",
      "query: 216 ,AP: 0.828987746145\n",
      "query: 217 ,AP: 0.999641622797\n",
      "query: 218 ,AP: 0.606060606061\n",
      "query: 219 ,AP: 0.337907550056\n",
      "query: 220 ,AP: 0.613822662115\n",
      "query: 221 ,AP: 0.397614314115\n",
      "query: 222 ,AP: 0.69352079397\n",
      "query: 223 ,AP: 0.994074673605\n",
      "query: 224 ,AP: 0.517873237873\n",
      "query: 225 ,AP: 0.992006355326\n",
      "MAP = 0.7653112644\n",
      "query 171 , NDCG:  0.9398543518229351\n",
      "query 172 , NDCG:  0.9633915005168991\n",
      "query 173 , NDCG:  0.8787194969898994\n",
      "query 174 , NDCG:  0.4307012038436227\n",
      "query 175 , NDCG:  0.8270509709611499\n",
      "query 176 , NDCG:  0.7642638365304593\n",
      "query 177 , NDCG:  0.4031227315582503\n",
      "query 178 , NDCG:  0.9149546965165309\n",
      "query 179 , NDCG:  0.9092261961802077\n",
      "query 180 , NDCG:  0.5611568950054991\n",
      "query 181 , NDCG:  0.9083280342057781\n",
      "query 182 , NDCG:  0.8840328408126277\n",
      "query 183 , NDCG:  0.9102744814242358\n",
      "query 184 , NDCG:  0.8445138195987467\n",
      "query 185 , NDCG:  0.5651704753561145\n",
      "query 186 , NDCG:  0.9174314725664987\n",
      "query 187 , NDCG:  0.8568815395907531\n",
      "query 188 , NDCG:  0.834462410887587\n",
      "query 189 , NDCG:  0.11401721726142679\n",
      "query 190 , NDCG:  0.9087219839232467\n",
      "query 191 , NDCG:  0.8333343147042753\n",
      "query 192 , NDCG:  0.8691210155951211\n",
      "query 193 , NDCG:  0.870741244990849\n",
      "query 194 , NDCG:  0.9169177532845512\n",
      "query 195 , NDCG:  0.6639616560278855\n",
      "query 196 , NDCG:  0.985504817367503\n",
      "query 197 , NDCG:  0.9366145863919296\n",
      "query 198 , NDCG:  0.8656740779203047\n",
      "query 199 , NDCG:  0.8277140391541248\n",
      "query 200 , NDCG:  0.8711501907272423\n",
      "query 201 , NDCG:  0.88743362829229\n",
      "query 202 , NDCG:  0.9107545852790405\n",
      "query 203 , NDCG:  0.5568543671092813\n",
      "query 204 , NDCG:  0.8819018257589796\n",
      "query 205 , NDCG:  0.9252555622479198\n",
      "query 206 , NDCG:  0.8077691566644618\n",
      "query 207 , NDCG:  0.8228677166265421\n",
      "query 208 , NDCG:  0.8581807002957024\n",
      "query 209 , NDCG:  0.6939250586840917\n",
      "query 210 , NDCG:  0.9144104200186212\n",
      "query 211 , NDCG:  0.046597135518310455\n",
      "query 212 , NDCG:  0.8282008992831403\n",
      "query 213 , NDCG:  0.9622674183267285\n",
      "query 214 , NDCG:  0.7936758103546756\n",
      "query 215 , NDCG:  0.6952067228508936\n",
      "query 216 , NDCG:  0.8590276180495913\n",
      "query 217 , NDCG:  0.9215234698355937\n",
      "query 218 , NDCG:  0.9229337070288905\n",
      "query 219 , NDCG:  0.498155912259978\n",
      "query 220 , NDCG:  0.5674800702438964\n",
      "query 221 , NDCG:  0.9112435588714188\n",
      "query 222 , NDCG:  0.618683715017237\n",
      "query 223 , NDCG:  0.9063275712084274\n",
      "query 224 , NDCG:  0.3773185814513307\n",
      "query 225 , NDCG:  0.9706077927297266\n",
      "NDCG = 0.78392023374\n"
     ]
    }
   ],
   "source": [
    "k = 200\n",
    "# query relevance file\n",
    "file_qrels_path = 'eval_hw4/qrels.txt'\n",
    "# qrels_dict = {query_id:{doc_id:gain, doc_id:gain, ...}, ...}\n",
    "qrels_dict = generate_tweetid_gain(file_qrels_path)\n",
    "# ur result, format is in function read_tweetid_test, or u can write by ur own\n",
    "file_test_path = 'eval_hw4/hisresult.txt'\n",
    "# test_dict = {query_id:[doc_id, doc_id, ...], ...}\n",
    "test_dict = read_tweetid_test(file_test_path)\n",
    "MAP = MAP_eval(qrels_dict, test_dict, k)\n",
    "print('MAP', ' = ', MAP, sep='')\n",
    "NDCG = NDCG_eval(qrels_dict, test_dict, k)\n",
    "print('NDCG', ' = ', NDCG, sep='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
