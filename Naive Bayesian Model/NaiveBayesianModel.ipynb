{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯分类器\n",
    "\n",
    "作者：周阳\n",
    "\n",
    "主要任务：\n",
    "\n",
    "* 分类文件整合\n",
    "* 基于朴素贝叶斯的概率统计\n",
    "  - 多项式构建\n",
    "  - 伯努力构建\n",
    "  - 混合模型构建\n",
    "* 每个类别构建词表（20个log词表）\n",
    "* 通过查表完成测试分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过文件夹分类文件并且整合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as ran\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 进度条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "class ProgressBar:\n",
    "    def __init__(self, count = 0, total = 0, width = 30):\n",
    "        self.count = count\n",
    "        self.total = total\n",
    "        self.width = width\n",
    "        print('-------------------------------Progressing----------------------------------')\n",
    "    def move(self):\n",
    "        self.count += 1\n",
    "    def log(self):\n",
    "        sys.stdout.write(' ' * (self.width + 9) + '\\r')\n",
    "        sys.stdout.flush()\n",
    "        progress = self.width * self.count / self.total\n",
    "        sys.stdout.write('{0:3}/{1:3} with {2} finished: '.format(self.count, self.total ,str(self.count/self.total*100)[:4]+'%'))\n",
    "        sys.stdout.write('#' * int(progress) + '-' * int(self.width - progress) + '\\r')\n",
    "        if progress == self.width:\n",
    "            sys.stdout.write('\\n')\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classis = {0:'/home/blueberry/data/datamining/20news-18828/alt.atheism',\n",
    "           1:'/home/blueberry/data/datamining/20news-18828/comp.graphics',\n",
    "           2:'/home/blueberry/data/datamining/20news-18828/comp.os.ms-windows.misc',\n",
    "           3:'/home/blueberry/data/datamining/20news-18828/comp.sys.ibm.pc.hardware',\n",
    "           \n",
    "           4:'/home/blueberry/data/datamining/20news-18828/comp.sys.mac.hardware',\n",
    "           5:'/home/blueberry/data/datamining/20news-18828/comp.windows.x',\n",
    "           6:'/home/blueberry/data/datamining/20news-18828/misc.forsale',\n",
    "           7:'/home/blueberry/data/datamining/20news-18828/rec.autos',\n",
    "           \n",
    "           8:'/home/blueberry/data/datamining/20news-18828/rec.motorcycles',\n",
    "           9:'/home/blueberry/data/datamining/20news-18828/rec.sport.baseball',\n",
    "           10:'/home/blueberry/data/datamining/20news-18828/rec.sport.hockey',\n",
    "           11:'/home/blueberry/data/datamining/20news-18828/sci.crypt',\n",
    "           \n",
    "           12:'/home/blueberry/data/datamining/20news-18828/sci.electronics',\n",
    "           13:'/home/blueberry/data/datamining/20news-18828/sci.med',\n",
    "           14:'/home/blueberry/data/datamining/20news-18828/sci.space',\n",
    "           15:'/home/blueberry/data/datamining/20news-18828/soc.religion.christian',\n",
    "           \n",
    "           16:'/home/blueberry/data/datamining/20news-18828/talk.politics.guns',\n",
    "           17:'/home/blueberry/data/datamining/20news-18828/talk.politics.mideast',\n",
    "           18:'/home/blueberry/data/datamining/20news-18828/talk.politics.misc',\n",
    "           19:'/home/blueberry/data/datamining/20news-18828/talk.religion.misc'\n",
    "          }\n",
    "save_path = '/home/blueberry/data/datamining/class_with_lable.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(f,result):\n",
    "    \n",
    "    fs = os.listdir(f)\n",
    "    for f1 in fs:\n",
    "        tmp_path = os.path.join(f,f1)\n",
    "        if not os.path.isdir(tmp_path):\n",
    "            result.append(tmp_path)\n",
    "        else:\n",
    "            print(tmp_path)\n",
    "            traverse(tmp_path,result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 注意这里使用的_pre文件是经过上次实验已经进行过去停用词，符号，获得词干等操作的文件结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_classis = list(classis.keys())\n",
    "text_with_lable = []\n",
    "for k in keys_classis:\n",
    "    cls_path = []\n",
    "    traverse(classis[k],cls_path)\n",
    "    for path in cls_path:\n",
    "        if path[-4:]=='_pre':\n",
    "            with open(path,'r+') as f :\n",
    "                lines = f.readlines()\n",
    "                lines = [line.strip('\\n') for line in lines]\n",
    "                lines.insert(0,k)##类别在当前list的第一个位置\n",
    "            text_with_lable.append(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  'liveseysolntzewpdsgicom',\n",
       "  'jon',\n",
       "  'livesey',\n",
       "  'subject',\n",
       "  'cruel',\n",
       "  'polit',\n",
       "  'atheist',\n",
       "  'articl',\n",
       "  'aprbmerhbnrca',\n",
       "  'dgrahambmersbnrca',\n",
       "  'dougla',\n",
       "  'graham',\n",
       "  'write',\n",
       "  'articl',\n",
       "  'qnedmafidoasdsgicom',\n",
       "  'liveseysolntzewpdsgicom',\n",
       "  'jon',\n",
       "  'livesey',\n",
       "  'write',\n",
       "  'articl',\n",
       "  'qlmdinngapcaltechedu',\n",
       "  'keithccocaltechedu',\n",
       "  'keith',\n",
       "  'allan',\n",
       "  'schneider',\n",
       "  'write',\n",
       "  'spent',\n",
       "  'quit',\n",
       "  'bit',\n",
       "  'time',\n",
       "  'word',\n",
       "  'constitut',\n",
       "  'realis',\n",
       "  'wide',\n",
       "  'held',\n",
       "  'belief',\n",
       "  'america',\n",
       "  'fact',\n",
       "  'claus',\n",
       "  'cruel',\n",
       "  'unusu',\n",
       "  'punish',\n",
       "  'like',\n",
       "  'lot',\n",
       "  'rest',\n",
       "  'lift',\n",
       "  'english',\n",
       "  'bill',\n",
       "  'right',\n",
       "  'accord',\n",
       "  'jerri',\n",
       "  'mander',\n",
       "  'absenc',\n",
       "  'sacr',\n",
       "  'good',\n",
       "  'book',\n",
       "  'btw',\n",
       "  'great',\n",
       "  'bind',\n",
       "  'law',\n",
       "  'iroquoi',\n",
       "  'confederaci',\n",
       "  'also',\n",
       "  'play',\n",
       "  'signific',\n",
       "  'role',\n",
       "  'model',\n",
       "  'us',\n",
       "  'constitut',\n",
       "  'furthermor',\n",
       "  'appar',\n",
       "  'marx',\n",
       "  'engel',\n",
       "  'strong',\n",
       "  'influenc',\n",
       "  'studi',\n",
       "  'iroquoi',\n",
       "  'societi',\n",
       "  'use',\n",
       "  'prime',\n",
       "  'exampl',\n",
       "  'success',\n",
       "  'classless',\n",
       "  'egalitarian',\n",
       "  'noncoerc',\n",
       "  'societi',\n",
       "  'mander',\n",
       "  'goe',\n",
       "  'say',\n",
       "  'us',\n",
       "  'ussr',\n",
       "  'would',\n",
       "  'well',\n",
       "  'studi',\n",
       "  'origin',\n",
       "  'document',\n",
       "  'figur',\n",
       "  'went',\n",
       "  'wrong',\n",
       "  'tri',\n",
       "  'get',\n",
       "  'right',\n",
       "  'next',\n",
       "  'time',\n",
       "  'fascin',\n",
       "  'heard',\n",
       "  'chines',\n",
       "  'rather',\n",
       "  'italian',\n",
       "  'invent',\n",
       "  'pasta',\n",
       "  'jon'],\n",
       " [0,\n",
       "  'frankdsuucp',\n",
       "  'frank',\n",
       "  'odwyer',\n",
       "  'subject',\n",
       "  'year',\n",
       "  'say',\n",
       "  'christian',\n",
       "  'moral',\n",
       "  'articl',\n",
       "  'aprcnsvaxuwecedu',\n",
       "  'nyedacnsvaxuwecedu',\n",
       "  'david',\n",
       "  'nye',\n",
       "  'write',\n",
       "  'repli',\n",
       "  'frankdsuucp',\n",
       "  'frank',\n",
       "  'odwyer',\n",
       "  'problem',\n",
       "  'objectivist',\n",
       "  'determin',\n",
       "  'status',\n",
       "  'moral',\n",
       "  'truth',\n",
       "  'method',\n",
       "  'establish',\n",
       "  'accept',\n",
       "  'judgement',\n",
       "  'report',\n",
       "  'onli',\n",
       "  'relat',\n",
       "  'ought',\n",
       "  'see',\n",
       "  'naturalist',\n",
       "  'fallaci',\n",
       "  'cannot',\n",
       "  'prove',\n",
       "  'ani',\n",
       "  'fact',\n",
       "  'natur',\n",
       "  'world',\n",
       "  'avoid',\n",
       "  'least',\n",
       "  'two',\n",
       "  'way',\n",
       "  'leav',\n",
       "  'good',\n",
       "  'undefin',\n",
       "  'sinc',\n",
       "  'anyon',\n",
       "  'claim',\n",
       "  'know',\n",
       "  'either',\n",
       "  'lie',\n",
       "  'touch',\n",
       "  'human',\n",
       "  'undeserv',\n",
       "  'repli',\n",
       "  'good',\n",
       "  'undefin',\n",
       "  'undefin',\n",
       "  'requir',\n",
       "  'everyon',\n",
       "  'know',\n",
       "  'innat',\n",
       "  'right',\n",
       "  'back',\n",
       "  'subjectiv',\n",
       "  'beg',\n",
       "  'question',\n",
       "  'see',\n",
       "  'defin',\n",
       "  'good',\n",
       "  'sole',\n",
       "  'term',\n",
       "  'evalu',\n",
       "  'term',\n",
       "  'ditto',\n",
       "  'evalu',\n",
       "  'statement',\n",
       "  'impli',\n",
       "  'valu',\n",
       "  'judgement',\n",
       "  'part',\n",
       "  'person',\n",
       "  'make',\n",
       "  'incorrect',\n",
       "  'questionbeg',\n",
       "  'see',\n",
       "  'point',\n",
       "  'objectivist',\n",
       "  'may',\n",
       "  'talk',\n",
       "  'selfevid',\n",
       "  'truth',\n",
       "  'pretti',\n",
       "  'percept',\n",
       "  'prof',\n",
       "  'flew',\n",
       "  'deni',\n",
       "  'subjectivist',\n",
       "  'claim',\n",
       "  'selfevid',\n",
       "  'mind',\n",
       "  'behold',\n",
       "  'course;',\n",
       "  'deni',\n",
       "  'subjectobject',\n",
       "  'true',\n",
       "  'dichotomi',\n",
       "  'pleas',\n",
       "  'explain',\n",
       "  'help',\n",
       "  'dont',\n",
       "  'see',\n",
       "  'argument',\n",
       "  'dont',\n",
       "  'see',\n",
       "  'seem',\n",
       "  'rest',\n",
       "  'assert',\n",
       "  'everyth',\n",
       "  'either',\n",
       "  'subject',\n",
       "  'object',\n",
       "  'noth',\n",
       "  'compel',\n",
       "  'dichotomi',\n",
       "  'might',\n",
       "  'well',\n",
       "  'divid',\n",
       "  'world',\n",
       "  'subjectobject',\n",
       "  'event',\n",
       "  'even',\n",
       "  'seem',\n",
       "  'sensibl',\n",
       "  'causat',\n",
       "  'exampl',\n",
       "  'event',\n",
       "  'subject',\n",
       "  'object',\n",
       "  'furthermor',\n",
       "  'subjectobject',\n",
       "  'true',\n",
       "  'dichotomi',\n",
       "  'ie',\n",
       "  'everyth',\n",
       "  'either',\n",
       "  'subject',\n",
       "  'object',\n",
       "  'statement',\n",
       "  'selfevid',\n",
       "  'truth',\n",
       "  'mind',\n",
       "  'behold',\n",
       "  'accord',\n",
       "  'relativist',\n",
       "  'hard',\n",
       "  'compel',\n",
       "  'add',\n",
       "  'fact',\n",
       "  'world',\n",
       "  'quick',\n",
       "  'shove',\n",
       "  'entireti',\n",
       "  'subject',\n",
       "  'categori',\n",
       "  'idealist',\n",
       "  'solipsist',\n",
       "  'argument',\n",
       "  'perfect',\n",
       "  'good',\n",
       "  'altern',\n",
       "  'set',\n",
       "  'categori',\n",
       "  'subject',\n",
       "  'object',\n",
       "  'event',\n",
       "  'reduc',\n",
       "  'subject',\n",
       "  'object',\n",
       "  'qualiti',\n",
       "  'without',\n",
       "  'ani',\n",
       "  'logic',\n",
       "  'difficulti',\n",
       "  'whi',\n",
       "  'yes',\n",
       "  'guess',\n",
       "  'deni',\n",
       "  'selfevid',\n",
       "  'truth',\n",
       "  'mind',\n",
       "  'behold',\n",
       "  'left',\n",
       "  'claim',\n",
       "  'moral',\n",
       "  'judgement',\n",
       "  'true',\n",
       "  'noth',\n",
       "  'moral',\n",
       "  'judgement',\n",
       "  'true',\n",
       "  'thing',\n",
       "  'common',\n",
       "  'refer',\n",
       "  'nihil',\n",
       "  'entail',\n",
       "  'scienc',\n",
       "  'valu',\n",
       "  'irrepect',\n",
       "  'fact',\n",
       "  'peopl',\n",
       "  'find',\n",
       "  'use',\n",
       "  'anyon',\n",
       "  'arriv',\n",
       "  'relativismsubjectiv',\n",
       "  'argument',\n",
       "  'beat',\n",
       "  'make',\n",
       "  'sens',\n",
       "  'either',\n",
       "  'flew',\n",
       "  'argu',\n",
       "  'objectivist',\n",
       "  'wind',\n",
       "  'subjectivist',\n",
       "  'furthermor',\n",
       "  'nihilist',\n",
       "  'believ',\n",
       "  'noth',\n",
       "  'except',\n",
       "  'scienc',\n",
       "  'materi',\n",
       "  'revolut',\n",
       "  'peopl',\n",
       "  'im',\n",
       "  'refer',\n",
       "  'ethic',\n",
       "  'nihil',\n",
       "  'subjectivist',\n",
       "  'may',\n",
       "  'well',\n",
       "  'feel',\n",
       "  'remain',\n",
       "  'moral',\n",
       "  'judgement',\n",
       "  'would',\n",
       "  'wish',\n",
       "  'associ',\n",
       "  'hold',\n",
       "  'moral',\n",
       "  'opinion',\n",
       "  'suggest',\n",
       "  'know',\n",
       "  'someth',\n",
       "  'true',\n",
       "  'prefer',\n",
       "  'regard',\n",
       "  'human',\n",
       "  'activ',\n",
       "  'prefer',\n",
       "  'includ',\n",
       "  'terror',\n",
       "  'moral',\n",
       "  'opinion',\n",
       "  'true',\n",
       "  'likewis',\n",
       "  'prefer',\n",
       "  'includ',\n",
       "  'noterror',\n",
       "  'moral',\n",
       "  'opinion',\n",
       "  'true',\n",
       "  'whi',\n",
       "  'one',\n",
       "  'choos',\n",
       "  'set',\n",
       "  'prefer',\n",
       "  'includ',\n",
       "  'terrorisim',\n",
       "  'one',\n",
       "  'includ',\n",
       "  'noterror',\n",
       "  'oh',\n",
       "  'reason',\n",
       "  'patent',\n",
       "  'absurd',\n",
       "  'also',\n",
       "  'posit',\n",
       "  'subjectivist',\n",
       "  'point',\n",
       "  'alreadi',\n",
       "  'ditch',\n",
       "  'strawman',\n",
       "  'alreadi',\n",
       "  'see',\n",
       "  'repli',\n",
       "  'mike',\n",
       "  'cobb',\n",
       "  'root',\n",
       "  'messag',\n",
       "  'thread',\n",
       "  'societ',\n",
       "  'basi',\n",
       "  'moral',\n",
       "  'ive',\n",
       "  'respond',\n",
       "  'btw',\n",
       "  'dont',\n",
       "  'intend',\n",
       "  'strawman',\n",
       "  'someth',\n",
       "  'logic',\n",
       "  'entail',\n",
       "  'relativ',\n",
       "  'realli',\n",
       "  'ani',\n",
       "  'ethic',\n",
       "  'system',\n",
       "  'valu',\n",
       "  'assum',\n",
       "  'unreal',\n",
       "  'differ',\n",
       "  'say',\n",
       "  'relativist',\n",
       "  'say',\n",
       "  'relativ',\n",
       "  'impli',\n",
       "  'frank',\n",
       "  'odwyer',\n",
       "  'im',\n",
       "  'hatch',\n",
       "  'odwyerssei',\n",
       "  'hen',\n",
       "  'evelyn',\n",
       "  'conlon']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_with_lable[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 乱序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12,\n",
       " 'weswsrhpcom',\n",
       " 'wes',\n",
       " 'whiteley',\n",
       " 'subject',\n",
       " 'solvent',\n",
       " 'ducttap',\n",
       " 'adhes',\n",
       " 'use',\n",
       " 'product',\n",
       " 'call',\n",
       " 'goofoff',\n",
       " 'come',\n",
       " 'littl',\n",
       " 'yellow',\n",
       " 'size',\n",
       " 'deck',\n",
       " 'play',\n",
       " 'card',\n",
       " 'work',\n",
       " 'well',\n",
       " 'remov',\n",
       " 'kind',\n",
       " 'sticker',\n",
       " 'tape',\n",
       " 'residu',\n",
       " 'note',\n",
       " 'alway',\n",
       " 'test',\n",
       " 'small',\n",
       " 'area',\n",
       " 'inconspicu',\n",
       " 'place',\n",
       " 'befor',\n",
       " 'use',\n",
       " 'good',\n",
       " 'luck',\n",
       " 'wes',\n",
       " 'whiteley',\n",
       " 'weswsrhpcom']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran.shuffle(text_with_lable)\n",
    "text_with_lable[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存\n",
    "通过npy文件序列化保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(save_path,text_with_lable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多项式朴素贝叶斯模型\n",
    "\n",
    "- P(Wi|S) = （有Wi的文档S出现次数 + 1） / (S类文档所有词 + len(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_polynomial_list =  [\n",
    "    {},{},{},{},{}, \n",
    "    {},{},{},{},{},\n",
    "    {},{},{},{},{}, \n",
    "    {},{},{},{},{},\n",
    "              ]##每一类class中的每个词的Prob统计\n",
    "vocabs_class =  [\n",
    "    {},{},{},{},{}, \n",
    "    {},{},{},{},{},\n",
    "    {},{},{},{},{}, \n",
    "    {},{},{},{},{},\n",
    "              ]##每个类别中出现的词表统计\n",
    "vocab_class_len = [] ##每一类中出现的词（一个词出现j次则统计j次）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##上次储存的vocab文件 格式:{(str)wi:(int)出现次数}\n",
    "vocab = np.load('/home/blueberry/data/datamining/vocab.npy')\n",
    "vocab = dict(vocab.item())\n",
    "keys_vocab = list(vocab.keys())\n",
    "\n",
    "for dicts in vocab_polynomial_list:\n",
    "    for k in keys_vocab:\n",
    "        dicts[k.decode(encoding='utf-8')] = 0.0 ##初始每个类别字典\n",
    "for dicts in vocabs_class:\n",
    "    for k in keys_vocab:\n",
    "        dicts[k.decode(encoding='utf-8')] = 0##初始每个类别字典，这让李decode变成字符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 统计类别词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------Progressing----------------------------------\n",
      "18828/18828 with 100.% finished: ##############################\n"
     ]
    }
   ],
   "source": [
    "bar = ProgressBar(total = 18828)\n",
    "for i,k in enumerate(keys_classis):\n",
    "    cls_path = []\n",
    "    traverse(classis[k],cls_path)\n",
    "    l_num = 0\n",
    "    for path in cls_path:\n",
    "        if path[-4:]=='_pre':\n",
    "            with open(path,'r+') as f :\n",
    "                bar.move()\n",
    "                if bar.count%100 == 0:\n",
    "                    bar.log()\n",
    "                lines = f.readlines()\n",
    "                lines = [line.strip('\\n') for line in lines]\n",
    "                l_num += len(lines)\n",
    "                for l in lines:\n",
    "                    try:\n",
    "                        vocabs_class[i][l] += 1 ## 统计值自加改变 vocabs_class 中每个字典的值 \n",
    "                    except Exception:\n",
    "                        print('exception',l)## 这里防止出错\n",
    "    vocab_class_len.append(l_num)\n",
    "bar.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 所有文档的总词数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2756184"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(vocab_class_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算Prob\n",
    "- 加入平滑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------Progressing----------------------------------\n",
      "133015/133015 with 100.% finished: ##############################\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "bar = ProgressBar(total = len(vocab))\n",
    "for wi in keys_vocab:##遍历词典中的每一词\n",
    "    for i,k in enumerate(keys_classis):##需要计算每一个词对于每一个分类的prob<可以理解为一种影响，加入平滑>\n",
    "        vocab_polynomial_list[i][wi.decode(encoding='utf-8')] = math.log((vocabs_class[i][wi.decode(encoding='utf-8')] + 1)/(vocab_class_len[i]+len(vocab)),2)##以为底的对数\n",
    "        ##存对数以便之后求概率× => log概率+\n",
    "    bar.move()\n",
    "    if bar.count%10000 == 0:\n",
    "        bar.log()\n",
    "bar.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.954523631593336"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_polynomial_list[0][keys_vocab[10].decode(encoding='utf-8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(vocab,reverse = True):##默认从大到小排序\n",
    "    v_tuple = sorted(vocab.items(),key = lambda x:x[1],reverse=reverse) ##从大到小排序\n",
    "    vocab = {}\n",
    "    for t in v_tuple:\n",
    "        vocab[t[0]] = t[1]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for voc in vocab_polynomial_list:\n",
    "    voc = sort_dict(voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打印每一类 top5词汇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0 类top5： \n",
      " [('subject', -8.167850118172208), ('one', -7.955855145338879), ('use', -9.200622317889138), ('write', -8.062258083141678), ('would', -8.331040285585793)] \n",
      "\n",
      "第 1 类top5： \n",
      " [('subject', -7.994443321600229), ('one', -9.057838402888738), ('use', -7.923412082667811), ('write', -8.8804188648995), ('would', -8.91063347794651)] \n",
      "\n",
      "第 2 类top5： \n",
      " [('subject', -7.936060847399895), ('one', -9.123764845641242), ('use', -7.772463127498815), ('write', -8.738228079434506), ('would', -9.252456068618429)] \n",
      "\n",
      "第 3 类top5： \n",
      " [('subject', -7.836296965380013), ('one', -8.634173141549553), ('use', -7.854729683476998), ('write', -8.972110820725733), ('would', -8.83348194977296)] \n",
      "\n",
      "第 4 类top5： \n",
      " [('subject', -7.806682252514311), ('one', -8.725025032945178), ('use', -8.126439248360047), ('write', -8.82124034820448), ('would', -8.85080480574662)] \n",
      "\n",
      "第 5 类top5： \n",
      " [('subject', -7.830200028615227), ('one', -9.071000291910222), ('use', -7.380989237372828), ('write', -9.065607033140003), ('would', -9.308328983551542)] \n",
      "\n",
      "第 6 类top5： \n",
      " [('subject', -7.71272565884888), ('one', -9.262204446626841), ('use', -8.966592789733257), ('write', -10.661901399771198), ('would', -9.956348758236453)] \n",
      "\n",
      "第 7 类top5： \n",
      " [('subject', -7.864221729412027), ('one', -8.653722986656835), ('use', -8.880900470558657), ('write', -8.212647626476466), ('would', -8.556737371287987)] \n",
      "\n",
      "第 8 类top5： \n",
      " [('subject', -7.846062410998222), ('one', -8.669075745721724), ('use', -9.558229977033433), ('write', -8.010864262969928), ('would', -9.031645825106432)] \n",
      "\n",
      "第 9 类top5： \n",
      " [('subject', -7.913357752163946), ('one', -8.687981472388676), ('use', -10.083282753606257), ('write', -8.172216482026547), ('would', -8.498320252885103)] \n",
      "\n",
      "第 10 类top5： \n",
      " [('subject', -8.027475076221245), ('one', -8.849648207318241), ('use', -11.066394065513547), ('write', -8.652766136489376), ('would', -8.700071851267731)] \n",
      "\n",
      "第 11 类top5： \n",
      " [('subject', -8.146683737891976), ('one', -8.355388666730471), ('use', -7.551113137604839), ('write', -8.432793170416076), ('would', -8.210632600432621)] \n",
      "\n",
      "第 12 类top5： \n",
      " [('subject', -7.770332014355819), ('one', -8.256818089033915), ('use', -7.653430113842575), ('write', -8.58432002874587), ('would', -8.409233322187777)] \n",
      "\n",
      "第 13 类top5： \n",
      " [('subject', -8.046222877408805), ('one', -8.318526581110143), ('use', -8.282319567953138), ('write', -8.599203947731185), ('would', -8.74798314030195)] \n",
      "\n",
      "第 14 类top5： \n",
      " [('subject', -8.075298859558174), ('one', -8.688911565036076), ('use', -8.743677368466967), ('write', -8.428635659164845), ('would', -8.144472945170033)] \n",
      "\n",
      "第 15 类top5： \n",
      " [('subject', -8.09942267129276), ('one', -7.790546272004267), ('use', -9.0884329817852), ('write', -8.62612900818552), ('would', -7.93509242707966)] \n",
      "\n",
      "第 16 类top5： \n",
      " [('subject', -8.210202630733795), ('one', -8.466787261366806), ('use', -8.54986900286136), ('write', -8.161097158276322), ('would', -7.910183315799364)] \n",
      "\n",
      "第 17 类top5： \n",
      " [('subject', -8.425360584762503), ('one', -8.123332047312845), ('use', -9.76052450680656), ('write', -8.42141340024584), ('would', -8.443924931516387)] \n",
      "\n",
      "第 18 类top5： \n",
      " [('subject', -8.492610814330753), ('one', -8.445635197950104), ('use', -9.111216158879545), ('write', -8.191985105172195), ('would', -8.0052363729047)] \n",
      "\n",
      "第 19 类top5： \n",
      " [('subject', -8.394146379631207), ('one', -8.240961233649003), ('use', -9.386284261858236), ('write', -8.460697572539813), ('would', -8.629075447784647)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,voc in enumerate(vocab_polynomial_list):\n",
    "    \n",
    "    top5 = []\n",
    "    keys_voc = list(voc.keys())\n",
    "    for j in range(5):\n",
    "        top5.append((keys_voc[j],voc[keys_voc[j]]))\n",
    "    print('第',i,'类top5：','\\n',top5,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 存储所得的类向量表存储到文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(open('vocab_polynomial_list.bin','wb'),vocab_polynomial_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型测试\n",
    "- test 数据理论上不应在 train set中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = text_with_lable[:1000] ##取100个已经shuffle的文件作为test set\n",
    "X = [t[1:]  for t in test] ##data\n",
    "Y = [t[0] for t in test]##lables\n",
    "result = [] ##判断对计为1 否则计为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分类过程\n",
    "- 其实就是简单的查表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = []\n",
    "for i,x in enumerate(X):\n",
    "    prob = [0,0,0,0,0,\n",
    "           0,0,0,0,0,\n",
    "           0,0,0,0,0,\n",
    "           0,0,0,0,0,]\n",
    "    for j in range(len(classis)):\n",
    "        for l in x:##对每个词查表求和\n",
    "            prob[j] += vocab_polynomial_list[j][l]\n",
    "    predict.append(np.argmax(prob))##找最大的位置作为predict,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    result.append(1 if predict[i] == Y[i] else 0)##判断predict==lable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc  =>   95.8 %\n"
     ]
    }
   ],
   "source": [
    "print('acc  =>  ',sum(result)/len(result)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 伯努力朴素贝叶斯模型\n",
    "\n",
    "- P(Wi|S) = （有Wi的文档S数 + 1） / (S类文档出现的词（一个词一个文档只计算一次） + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_bernoulli_list =  [\n",
    "    {},{},{},{},{}, \n",
    "    {},{},{},{},{},\n",
    "    {},{},{},{},{}, \n",
    "    {},{},{},{},{},\n",
    "              ]##每一类class中的每个词的Prob统计\n",
    "vocabs_class =  [\n",
    "    {},{},{},{},{}, \n",
    "    {},{},{},{},{},\n",
    "    {},{},{},{},{}, \n",
    "    {},{},{},{},{},\n",
    "              ]##每个类别中出现的词表统计\n",
    "vocab_class_len = [] ##每一类中出现的词（一个词在一个文档中至多计算一次）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dicts in vocab_bernoulli_list:\n",
    "    for k in keys_vocab:\n",
    "        dicts[k.decode(encoding='utf-8')] = 0.0 ##初始每个类别字典\n",
    "for dicts in vocabs_class:\n",
    "    for k in keys_vocab:\n",
    "        dicts[k.decode(encoding='utf-8')] = 0##初始每个类别字典，这让李decode变成字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------Progressing----------------------------------\n",
      "18828/18828 with 100.% finished: ##############################\n"
     ]
    }
   ],
   "source": [
    "bar = ProgressBar(total = 18828)\n",
    "for i,k in enumerate(keys_classis):\n",
    "    cls_path = []\n",
    "    traverse(classis[k],cls_path)\n",
    "    l_num = 0\n",
    "    for path in cls_path:\n",
    "        if path[-4:]=='_pre':\n",
    "            with open(path,'r+') as f :\n",
    "                bar.move()\n",
    "                if bar.count%100 == 0:\n",
    "                    bar.log()\n",
    "                lines = f.readlines()\n",
    "                lines = [line.strip('\\n') for line in lines]\n",
    "                lines = set(lines)##对同一个document进行去重，set中的元素不能重复\n",
    "                l_num += len(lines)##这里统计的值也不会统计重复的\n",
    "                for l in lines:\n",
    "                    try:\n",
    "                        vocabs_class[i][l] += 1 ## 统计值自加改变 vocabs_class 中每个字典的值 \n",
    "                    except Exception:\n",
    "                        print('exception',l)## 这里防止出错\n",
    "    vocab_class_len.append(l_num)\n",
    "bar.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 所有文档的总词数\n",
    "- 明显比之前的 200w+ 少了很多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1834133"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(vocab_class_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------Progressing----------------------------------\n",
      "133015/133015 with 100.% finished: ##############################\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "bar = ProgressBar(total = len(vocab))\n",
    "for wi in keys_vocab:##遍历词典中的每一词\n",
    "    for i,k in enumerate(keys_classis):##需要计算每一个词对于每一个分类的prob<可以理解为一种影响，加入平滑>\n",
    "        vocab_bernoulli_list[i][wi.decode(encoding='utf-8')] = math.log((vocabs_class[i][wi.decode(encoding='utf-8')] + 1)/(vocab_class_len[i]+2),2)##以为底的对数\n",
    "        ##存对数以便之后求概率× => log概率+\n",
    "    bar.move()\n",
    "    if bar.count%10000 == 0:\n",
    "        bar.log()\n",
    "bar.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.290813885635139"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_bernoulli_list[0][keys_vocab[10].decode(encoding='utf-8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for voc in vocab_bernoulli_list:\n",
    "    voc = sort_dict(voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打印top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0 类top5： \n",
      " [('subject', -6.7967048153650955), ('one', -7.740121286998728), ('use', -8.685673502976352), ('write', -7.107405654829204), ('would', -7.97703663186864)] \n",
      "\n",
      "第 1 类top5： \n",
      " [('subject', -6.382106898559566), ('one', -8.281978864072023), ('use', -7.740029252310961), ('write', -7.721170225059645), ('would', -7.947941086906666)] \n",
      "\n",
      "第 2 类top5： \n",
      " [('subject', -6.4943383597192685), ('one', -8.269857194654868), ('use', -7.4972676907579405), ('write', -7.701689936476691), ('would', -8.389933646646618)] \n",
      "\n",
      "第 3 类top5： \n",
      " [('subject', -6.135601002453696), ('one', -7.6757691725120925), ('use', -7.218667613666705), ('write', -7.613124235523097), ('would', -7.6972702417230145)] \n",
      "\n",
      "第 4 类top5： \n",
      " [('subject', -6.033491687673114), ('one', -7.585832766825072), ('use', -7.439559033447404), ('write', -7.423748518599942), ('would', -7.625972157678287)] \n",
      "\n",
      "第 5 类top5： \n",
      " [('subject', -6.4621363908841865), ('one', -8.457731211764186), ('use', -7.420106139464268), ('write', -8.038301943368184), ('would', -8.312782875853085)] \n",
      "\n",
      "第 6 类top5： \n",
      " [('subject', -5.8562415753640185), ('one', -8.05461711558193), ('use', -7.793852883372964), ('write', -9.182624727958002), ('would', -8.74911456860768)] \n",
      "\n",
      "第 7 类top5： \n",
      " [('subject', -6.37230063616628), ('one', -7.825195996269563), ('use', -8.105873362890607), ('write', -7.21913337478161), ('would', -7.75518627502182)] \n",
      "\n",
      "第 8 类top5： \n",
      " [('subject', -6.325403772234103), ('one', -7.816350937582118), ('use', -8.509169428063942), ('write', -6.915450026157424), ('would', -8.08428414282875)] \n",
      "\n",
      "第 9 类top5： \n",
      " [('subject', -6.39740052543126), ('one', -7.86811320703922), ('use', -8.989631026616456), ('write', -7.146499875233321), ('would', -7.998401236244187)] \n",
      "\n",
      "第 10 类top5： \n",
      " [('subject', -6.566799935628512), ('one', -8.040731123960924), ('use', -9.962728611959651), ('write', -7.463805942305187), ('would', -8.0528039562615)] \n",
      "\n",
      "第 11 类top5： \n",
      " [('subject', -6.849518605140484), ('one', -7.996359993469755), ('use', -7.786906627840805), ('write', -7.439580260519307), ('would', -7.930270803011982)] \n",
      "\n",
      "第 12 类top5： \n",
      " [('subject', -6.250920677787469), ('one', -7.5075053084204795), ('use', -7.173691604415608), ('write', -7.26272193001982), ('would', -7.525163974916985)] \n",
      "\n",
      "第 13 类top5： \n",
      " [('subject', -6.712261496899738), ('one', -7.873839855531209), ('use', -8.276985458741093), ('write', -7.548658782848759), ('would', -8.145366491243013)] \n",
      "\n",
      "第 14 类top5： \n",
      " [('subject', -6.713703049135238), ('one', -8.103649567451251), ('use', -8.231617729054385), ('write', -7.407041710901186), ('would', -7.835521793429001)] \n",
      "\n",
      "第 15 类top5： \n",
      " [('subject', -6.961023350217656), ('one', -7.781812298252367), ('use', -8.724247010718551), ('write', -7.697507162766131), ('would', -7.807575394317448)] \n",
      "\n",
      "第 16 类top5： \n",
      " [('subject', -6.929477665054082), ('one', -8.067297951356808), ('use', -8.347156979831961), ('write', -7.477696555832131), ('would', -7.812417677271456)] \n",
      "\n",
      "第 17 类top5： \n",
      " [('subject', -7.3092431607383235), ('one', -8.386394173546556), ('use', -9.298550824568602), ('write', -7.738145428091424), ('would', -8.490326547232574)] \n",
      "\n",
      "第 18 类top5： \n",
      " [('subject', -7.074651860728015), ('one', -8.17073896491939), ('use', -8.685880016142978), ('write', -7.443343522203958), ('would', -8.06353990560779)] \n",
      "\n",
      "第 19 类top5： \n",
      " [('subject', -6.8690702949084965), ('one', -7.848573888022916), ('use', -8.739721747085687), ('write', -7.361855480604467), ('would', -8.094524139231162)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,voc in enumerate(vocab_bernoulli_list):\n",
    "    \n",
    "    top5 = []\n",
    "    keys_voc = list(voc.keys())\n",
    "    for j in range(5):\n",
    "        top5.append((keys_voc[j],voc[keys_voc[j]]))\n",
    "    print('第',i,'类top5：','\\n',top5,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型测试\n",
    "- 过程同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [] ##判断对计为1 否则计为0\n",
    "predict = []\n",
    "for i,x in enumerate(X):\n",
    "    prob = [0,0,0,0,0,\n",
    "           0,0,0,0,0,\n",
    "           0,0,0,0,0,\n",
    "           0,0,0,0,0,]\n",
    "    for j in range(len(classis)):\n",
    "        for l in x:##对每个词查表求和\n",
    "            prob[j] += vocab_bernoulli_list[j][l]\n",
    "    predict.append(np.argmax(prob))##找最大的位置作为predict,\n",
    "for i in range(len(X)):\n",
    "    result.append(1 if predict[i] == Y[i] else 0)##判断predict==lable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc  =>   94.6 %\n"
     ]
    }
   ],
   "source": [
    "print('acc  =>  ',sum(result)/len(result)*100,'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
